{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc1QaKOIo2a88hxWOz4tOh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi1994-tim/GenAI_Prompt_Engg./blob/main/GenAI_self.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSwU05UiLRh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89106b91-7e97-437a-fa0a-8170b33cd943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.32.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.32.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount= True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPHOMfCTVY3c",
        "outputId": "2d8e6f89-cc00-40d2-ce4e-07adc01eef03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = '/content/drive/MyDrive/Colab Notebooks'"
      ],
      "metadata": {
        "id": "kzT8WIBtWkqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/MyDrive/Colab Notebooks'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr4rBlilXTRq",
        "outputId": "57a1a90c-44b0-4c7f-e343-95ae665bae83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Bootcamp_Python_Code.ipynb\t\t     OverAPI.ipynb\n",
            "'_Business Case Scaler - Clustering.ipynb'   practice_session_AD1.ipynb\n",
            " Class04_Loss_Minimisation.ipynb\t     Scaler_Masterclass_python.ipynb\n",
            " Class05_Practice.ipynb\t\t\t     Untitled0.ipynb\n",
            "'Copy of Welcome to Colaboratory'\t     Untitled1.ipynb\n",
            "'Copy of Welcome to Colaboratory (1)'\t     Untitled2.ipynb\n",
            " GenAI_self.ipynb\t\t\t     Untitled3.ipynb\n",
            " Logistic_heartDisease.ipynb\t\t     Untitled4.ipynb\n",
            " Mathsfor_ML_Practice_Assingments.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os"
      ],
      "metadata": {
        "id": "aMV0Y6h5XoKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# openai.api_key = \"acbs1234\""
      ],
      "metadata": {
        "id": "h2pXhhWcXqWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qakr5CYZmWrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won IPL 2023\"}\n",
        "    ]"
      ],
      "metadata": {
        "id": "OWKYnN66Yd2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages\n",
        ")"
      ],
      "metadata": {
        "id": "Vy_Z-hqfZXq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TULo-6tiZu-_",
        "outputId": "0ff65705-4cfa-4204-f4c7-fad295835fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9XNSc0jwcFSjZubTB0I4Ym7WdUIXd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry, but I am an AI language model and I do not have real-time information. However, as of now, the IPL 2023 season has not taken place yet, so I am unable to provide you with the winner.\", role='assistant', function_call=None, tool_calls=None))], created=1717742034, model='gpt-3.5-turbo-16k-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=49, prompt_tokens=23, total_tokens=72))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "8093h9xsaiWu",
        "outputId": "03bf2497-9b55-4124-941f-aedd27995f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, but I am an AI language model and I do not have real-time information. However, as of now, the IPL 2023 season has not taken place yet, so I am unable to provide you with the winner.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages1=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who is the prime minister of India in 2019\"}\n",
        "    ]"
      ],
      "metadata": {
        "id": "pEY89mm6auPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response1 = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages1\n",
        ")"
      ],
      "metadata": {
        "id": "s4gZfX0obvVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response1.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "53LDftF8b0Bg",
        "outputId": "aec170da-455d-400e-c6b2-e4eb5168b60a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Prime Minister of India in 2019 was Narendra Modi.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages2=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who is the winner of IPL 2020\"}\n",
        "    ]"
      ],
      "metadata": {
        "id": "MOuVpwWkb4Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response2 = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages2\n",
        ")"
      ],
      "metadata": {
        "id": "6c_zzJBTcoZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTaNSq5Xcs73",
        "outputId": "a7fe328b-3c86-450c-a2f3-ebbb578de946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9XNeYy4OVB1rpnRHL1BgnoFA4os6p', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Mumbai Indians (MI) were the winners of the 2020 Indian Premier League (IPL).', role='assistant', function_call=None, tool_calls=None))], created=1717742774, model='gpt-3.5-turbo-16k-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=21, prompt_tokens=26, total_tokens=47))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response2.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Aq68w61Mcp-l",
        "outputId": "0758b770-4fd1-4df8-e234-845ee0f9232e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Mumbai Indians (MI) were the winners of the 2020 Indian Premier League (IPL).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt **Questioning**"
      ],
      "metadata": {
        "id": "U8B5mfe1fh_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''You are a helpful Neural Network teaching assistant.\n",
        "Explain the various optimisation methods in Neural network.\n",
        "Provide an exhaustive summary of the methods describing what they do,\n",
        "sample code for each, and guidelines on when to use which method.\n",
        "'''"
      ],
      "metadata": {
        "id": "kYZ79exTfkhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": prompt\n",
        "}]"
      ],
      "metadata": {
        "id": "F-wDSnjYfu7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    messages=message,\n",
        "    max_tokens=200, #using the number of tokens as o/p\n",
        "    temperature=0.5, #tuning for repetating words in the feild of answer\n",
        "    n=1, #shows the number of o/p an answer you want\n",
        "    stop=None, #checks for repetating words\n",
        "    frequency_penalty=0, #both works with the no. of frequency itmes/ words / tokens neing used\n",
        "    presence_penalty=0)"
      ],
      "metadata": {
        "id": "vtIvGgF2fzrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "6ZfHEbutgsPr",
        "outputId": "5b73ab26-379b-45a2-bb60-687e50c01e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"There are several optimization methods commonly used in neural networks to update the model parameters during the training process. These methods aim to minimize the loss function and improve the model's performance. Here is an exhaustive summary of some popular optimization methods, along with sample code and guidelines on when to use each method:\\n\\n1. Gradient Descent (GD):\\nGradient Descent is a basic optimization method that updates the model parameters in the opposite direction of the gradient of the loss function. It follows a fixed learning rate for all parameters.\\nCode:\\n```\\n# Gradient Descent\\nlearning_rate = 0.01\\n\\nfor epoch in range(num_epochs):\\n    # Forward pass\\n    # Calculate loss\\n    # Backward pass\\n    # Update parameters\\n    parameters -= learning_rate * gradients\\n```\\nGuidelines:\\n- Use when the dataset is small or the model is simple.\\n- Suitable for convex loss functions.\\n\\n2. Stochastic Gradient Descent (SGD):\\nSGD is an extension of GD where the\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u8zGtnE0guw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Works with the specicfied output Like we wanted to have output in JSON format"
      ],
      "metadata": {
        "id": "N8aHcX_Bhig2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON to the key 'answer'.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won IPL 2020\"}\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "V-ypwtZChiTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = response.choices[0].message.content\n",
        "\n",
        "# Print the generated output\n",
        "print(output)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print the type of the output (usually a string)\n",
        "print(\"Output Type:\", type(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gezxWoJomALE",
        "outputId": "0424225f-611d-4e62-998b-c4d7ae300fb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"answer\": \"Mumbai Indians\"\n",
            "}\n",
            "\n",
            "\n",
            "Output Type: <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "json_output = json.loads(output)\n",
        "\n",
        "result = json_output.get('answer', \"None\")\n",
        "\n",
        "# The 'result' variable now contains the value associated with the 'answer' key or \"None\" if the key is not present.\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ltu2EeHmOB5",
        "outputId": "be344f90-dc55-4324-f4a8-8938713eccb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mumbai Indians\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O5HTjYz2mhy8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}